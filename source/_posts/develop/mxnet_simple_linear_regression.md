---
title: mxnet之简单线性回归源码解读
date: 2017-12-12 10:40:21
updated: 2017-12-12 10:40:21
categories:
- 机器学习

tags:
- MXNet
---
# 前言
此篇博文相关：[线性回归 — 使用Gluon](http://zh.gluon.ai/chapter_supervised-learning/linear-regression-gluon.html)中源码的解读。
<!-- more -->
# 源码及注释解析
```python
# 设置迭代次数5次，每次迭代都完整取所有数据
epochs = 5
# 设置每次取数据的batch = 10，如例子中数据集是1000，则每次迭代都取100次数据，每次都取10个数据
batch_size = 10
# for循环迭代：共5次，range(epochs)模拟0，1，2，3，4
for e in range(epochs):
    #每次迭代，初始的损失置为0
    total_loss = 0
    # 从data_iter迭代器中取数据，data_iter = gluon.data.Dataloader(dataset,batch_size,shuffle=True)
    # 上诉dataset = gluon.data.ArrayDataset(X,y)即由数据集X和其对应的标签y组成的数据集合
    # 因此，data_iter迭代器每次取出的data和label即batch_size个数据集对：X'和y'，直到取完所有数据，结束循环（1000/10=100次）
    for data, label in data_iter:
        # 利用autograd.record()API自动记录梯度
        with autograd.record():
            # 输出等于将数据X传入你定义的模型net（可以这么理解：线性模型即是y'关于x的函数）
            output = net(data)
            # 这里定义的损失函数是衡量真值label与有你的模型产生的预测值之间的误差。此处使用的是均方误差，你也可以换成你自己定义的其他损失函数
            loss = square_loss(output, label)
        # 反向传播误差，这里可以将这个动作看作是一个信号，损失反向了，但是真正利用损失反向传播的却是接下来的trainer
        loss.backward()
        # 传播了误差之后，我们还需要利用一些更新参数的方法对参数进行更新
        # 注意这里使用的是gluon提供的API:gluon.Trainer(net.collect_params,'使用的更新模型参数的方法名：如随机梯度下降',{指定的学习率learning_rate:0.1是一个字典})
        # 如：trainer = gluon.Trainer(net.collect_params(),'sgd', {'learning_rate': 0.1})
        # 最后利用trainer.step(batch_size)API反向更新模型参数w,b
        trainer.step(batch_size)
        # 将每次训练的损失加入总损失（一共要加100次，直到迭代完成）。由于每次循环得到的是一个长度为batch_size的一维向量，但是总损失是标量。因此得利用asscalar()转化为标量进行操作
        total_loss += nd.sum(loss).asscalar()
    # 打印出一次迭代的平均误差=总误差/样本数
    print("Epoch %d, average loss: %f" % (e, total_loss/num_examples))
```
# 一些易混淆的名词
## 模型定义
可以这么通俗的理解：模型即是你定义的一个由数据集生成预测值的函数。在训练过程中，我们往模型中传入训练数据集得到的是一个预测值。我们无法知道这个预测值到底是对是错是好是坏。它只是单纯的对预测值的一个刻画。
## 损失函数
那么我们如何描述我们的模型是好是坏呢？这里就必须引入损失这个概念。所谓损失，单纯来讲就是预测值与真实值之间的误差。理想情况下，我们希望我们的损失是0或者接近于0。
## 初始化模型参数
其实当我们在说模型的时候，我们通常不能很完整的说出：我们的模型具体是什么（我在想这可能是由于我们维度限制了我们的想象力= =）。也就是大部分模型无法解释清楚的。不像线性模型：我们可以这么定义：$y^'=wx+b$，然后我们可以这么解释：例如，w是斜率而b是截距。但是对于一些非线性的模型，里头的参数不再具有可解释性质。
因此，一般情况下，我们说我训练一个模型指的就是我训练了模型中的参数。例如：w和b就是我们通过训练线性模型得到的参数。
但是，想象一下，最初的时候，我们手上只有一些数据，以及数据对应的标签（真实值）。还是以线性模型为例子：我们定义了一个模型$y^'=wx+b$(目前，在不对w和b初始化的情况下，这两个也是变量)。因此，我们的模型就有了三个变量。但是我们只希望在给模型传入一个数据x的时候得到的是一个预测值$y^'$。
因此，我们就需要初始化我们的模型参数w和b。我们可以随机对其初始化，也可以指定一个参数（可能是从其他地方迁移学习过来的：其实就是按照数据集的关系的相似性，衡量照抄它的参数= =）。
## 模型参数更新策略（优化）
现在，我们的模型中只剩下一个变量$y^'$，我们可以输入一个数据得到一个预测值。我们通过比较预测值和真实值得到损失。但是，我们要如何更新我们的参数使得我们的模型在每一次迭代完所有数据之后。根据损失，能够调整模型参数！实现利用**数据编程**！
这里就有了反向传播以及随机梯度下降等的概念。反向传播传播的是损失（也叫残差），随机梯度中就是利用损失的反向梯度，往回走batch_size个样本损失的距离，使得参数得到更新。`param[:] = param - lr * param.grad`。这里lr就是学习率。
这样在进行下一次循环（此时还是在一次迭代，100次循环里头）的时候，我们的参数能够有所改进，使模型更拟合真实数据。也就是说话我们共循环更新了5*1000/10=500次参数！
# 结束语
尽管不知道已经花了多少时间和精力取学习了多少机器学习相关的东西，但是感觉自己对于理论或者说这些专有名词，永远都不上心的样子。但是这样子真的是百害而无一利。直到我遇到了李沐老师团队的MXNet= =。决定一定要沉心静气学好理论并且动手实践！