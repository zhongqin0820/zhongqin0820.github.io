---
title: 读书笔记:Ian Goodfellow《深度学习》第六章
date: 2017-09-26 21:10:04
categories:
- 深度学习

tags:
- 学习总结
---
### 前言
&emsp;&emsp;今天看Ian Goodfellow的《深度学习》第六章，讲到神经网络的概念。其从前馈神经网络讲到基于微分的链式法则。然后讲到基于反向传播的神经网络的数学基础。老实说，看完之后，虽然笔记记了三、四页。但是还是很懵逼。在李飞飞的CS231n课程中也提到了关于反向神经网络是如何得出的。但是始终没能做到完全吸收理解。总觉得其理论支撑非常虚。
&emsp;&emsp;记得看Andrew采访Hilton的时候，也谈到了反向传播(Error <font color=red>B</font>ack <font color=red>P</font>ropagation)这个概念。这里暂时不追溯历史了。简单总结一下自己接下来学习到的关于反向传播的内容。可能有些地方理解不到位。后面也在慢慢修正自己的理解。

<!--more-->
### 基础概念
&emsp;&emsp;在分析算法之前，先来梳理一下几个算法中的基础概念。

#### 神经元
&emsp;&emsp;神经网络中的结点被称为神经元。他们之间通常通过带权重的边进行有向连接。

#### 输入层、隐藏层、输出层
&emsp;&emsp;由输入结点组成的层称为输入层。由输出结点组成的层称为输出层。中间的结点组成的各层统称为隐藏层(其对外界不可见，似乎被隐藏了起来)。

#### 激活函数
&emsp;&emsp;通常选用sigmoid函数，又称Logistic函数作为激活函数。关于sigmoid函数的性质，这篇[博客](http://www.cnblogs.com/startover/p/3143763.html)总结的不错。<br/>
&emsp;&emsp;其主要作用是，提供网络的非线性建模能力。否则你的网络中的结点之间都是线性关系，不论设置多少隐藏层学习的效果都是一样的。<br/>
&emsp;&emsp;关于激活函数的讨论有很多。这篇[文章](http://blog.csdn.net/u014595019/article/details/52562159)总结的很不错。其中还提到了一个代价函数的概念。

#### 损失函数或叫代价函数
&emsp;&emsp;BP算法中，提到了一个关于<font color=red>残差</font>的概念。发现很多地方都利用残差对算法进行优化。也有将其称为敏感度；残差可以看做是对偏置参数b的导数，偏置变化多少，残差就变化多少，残差就是偏置的变化率。当误差大的时候，权重更新就快，当误差小的时候，权重的更新就慢。

#### 正向传播、反向传播
&emsp;&emsp;正向传播时，输入样本从输入层进入网络，经隐层逐层传递至输出层，如果输出层的实际输出与期望输出(导师信号)不同，则转至误差反向传播；如果输出层的实际输出与期望输出(导师信号)相同，结束学习算法。<br/>
&emsp;&emsp;反向传播时，将输出误差(期望输出与实际输出之差)按原通路反传计算，通过隐层反向，直至输入层，在反传过程中将误差分摊给各层的各个单元，获得各层各单元的误差信号，并将其作为<font color=red>修正</font>各单元权值的根据。这一计算过程使用梯度下降法完成，在不停地调整各层神经元的权值和阈值后，<font color=red>使误差信号减小到最低限度</font>。

### 算法缺陷
1. 局部极小值
2. 权值过多
3. 算法终止策略
4. 过拟合

### 引用
1. [多层神经网络BP算法 原理及推导](http://www.cnblogs.com/liuwu265/p/4696388.html)
2. [BP算法与公式推导](http://blog.csdn.net/lu597203933/article/details/46575803)
3. [神经网络——BP算法](http://www.jianshu.com/p/c5cda5a52ee4)
4. [BP 算法之一种直观的解释](http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html)
5. [BP算法_百度百科](https://baike.baidu.com/item/BP%E7%AE%97%E6%B3%95/1252294?fr=aladdin)

### 结束语
&emsp;&emsp;阅读文章过程中，看到有说神经网络还有很多分支，如深度神经网络，脉冲神经网络等，并说脉冲神经网络被称为第三代神经网络。这里实在是不知道，留作以后再了解。<br/>
&emsp;&emsp;感觉可以这么理解，如果不引入反向传播，在正向传播中，你也可以得到一个答案。但是这个答案有可能不是最好的。因此，你可以利用这个结果误差进行反向指导。而反向指导所利用的一个重要的数学基础知识就是基于函数微分的链式规则(chain rule)。通过这个误差不断的修正权重矩阵的各个权重以及每层设置的偏置。<br/>
&emsp;&emsp;最后发现自己看书实在是囫囵吞枣，作者的千分之一都没能领悟啊。后续再添加程序实现以及相关数学推导。