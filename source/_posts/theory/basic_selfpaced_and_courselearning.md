---
title: 自步学习与课程学习基础概念
date: 2018-01-01 08:00:00
categories:
- 机器学习

tags:
- 基础概念
---
# 前言
此篇文章是关于SPL（自步学习-Self-Paced Learning）的基础概念的梳理。个人笔记。

<!-- more -->
# 课程学习与自步学习关系
现有机器学习方法都需要解决**非凸优化问题**。
课程学习和自步学习最开始就是作为解决非凸优化问题而提出的。
课程学习和自步学习的**核心思想是通过模拟人的认知机理**，首先学习简单的、普适性的知识结构，然后逐渐增加难度，过渡到学习更复杂、更专业化的知识。
> 这里应该比较疑惑的就是该怎么定义难与易？后面有解释：定义的目标、定义的方法！

课程学习是**对于特定问题而言**，根据先验知识或启示赋予样本不同的学习特性或学习先后顺序。如：根据先验知识对知识学习顺序的指定（instructor driven）。
相对于课程学习，自步学习则是对于**已学习获得的模型**而言，赋予样本不同的学习难易程度（easiness or confidance）。
从线性代数到高等数学的学习过程可以认为是自学习过程（student-driven），而线性代数在该过程被认为是简单样本，相对地，高等数学则被认为是复杂样本，**每次刷题的过程对应为已学习获得的模型**。
在机器学习研究中，我们也可以将学习对象(数据、特征、概念等)按其对学习目标的难易程度，**从易到难开展学习**，以这种方式让机器完成复杂的学习与推理任务。
# 自步学习的关键
自步学习研究的关键是**假设样本的选择并不是随机的，或是在一次迭代中全部纳入训练过程中**，而是**通过一种由简到难的有意义的方式进行选择的**。
自步学习中从简单到复杂的样本选择过程是指，简单样本可以理解为具有较小的损失（smaller loss）或较大似然函数值（likelihood）的样本，复杂的样本具有较大损失（larger loss）的样本。
> 注：此处十分好理解。根据损失函数的定义。损失越小，则预测值与目标值越接近。因此，也就被认为是越简单。这里关于最小损失和最大似然只是概念上的说法。但是二者所指可以说都是这么一个意思。


# 相关自步学习应用
1. SPMF算法对非凸矩阵分解问题
2. SpaR方法用于解决多模态多媒体事件检测问题
3. SP-MIL模型实现显著性检测
4. SPLD方法并应用于视频动作识别中
5. 物体跟踪
6. SPMoR模型将自步学习应用于混合模型的健壮估计中
> 此处将成为下一步的学习方向


# 自步学习优势
SPL优势在于可以针对特定任务设计不同的**自步正则项**，用于表征和定义“简单”样本。
最基本的SPL正则项为标准的**LASSO（norm）**，用于从所有样本中选择稀疏的、有竞争力的样本，即样本具有很小的训练误差、高似然值或者高置信度。
SPLD和SP-MIL分别**引用**和**范数**作为自步学习正则项，这两种正则项都属于**Group LASSO**。
和不同子集间选择变量，促使保留稀疏的子集，因此在自步学习中，二者都起到鼓励在多个子集中选择样本的作用，并避免子集间的稀疏性。

从健壮估计角度来看，自步学习理论本质上是一种健壮学习机制，自步学习定义的简单样本是指在学习过程的每次迭代中被选择的样本不会存在预测误差或代价超过一定阈值的样本，我们也称之为“置信度高”的样本。

SPMoR避免自步学习在每次迭代过程中在各成分间选择样本数量的不均衡。

# 自步学习核心内容
自步学习核心思想是在每次迭代过程中倾向于从所有样本中选择具有很小的训练误差、高似然值的样本，然后更新模型参数。
每次迭代选择样本的数量由权重参数确定，该参数通过逐次衰减实现引入更多的样本，当所有样本已被选择或者代价函数无法再降低则停止迭代。
自步学习在传统机器学习目标函数中引入二分变量（binary variable）$v_i$，用于表征每个样本是否被选择，或是否为简单样本。
## 双凸优化问题
最基本的SPL的优化过程很简单。当$f(x_i,y_i,w)≤1/K$时，$v_i=1$，当$f(x_i,y_i,w)>1/K$时，$v_i=0$。特别地，当$f(⋅)$和$r(⋅)$均为凸函数时，上述自步学习的优化问题可以转化为双凸优化问题（biconvex optimization problem）。
双凸优化问题是指对于待优化参数集合$z$而言，参数集合可以被划分为互斥的两个集合$z1$和$z2$。
如果任意一个参数集合$zi$固定一组参数值时，另一组参数的优化问题可以看做是凸优化问题，那么该问题可以视为双凸优化问题。
例如，在上述表达式中存在两组参数$w$和$v$，我们可以**交替固定**$w$求得$v$的最优解，然后固定$v$求$w$的最优解，该优化方法可以称为**alternative convex search (ACS)方法**，可以保证求得函数的局部最优解。

# 其他机器学习算法
机器学习方法中也存在相关方法**用于选择样本**，例如：主动学习（active learning）和协同训练（co-training）。
自步学习与二者的区别在于**自步学习中所有样本的标签是完全存在的**，在每次迭代过程中，我们可以通过计算预测值和标签的差别来选择置信度高的样本。
而主动学习和协同训练往往应用于半监督学习框架，其中主动学习倾向于选择当前模型下确定性或置信度低的样本，而协同训练则是从无标签数据中选择分类器认为置信度可能高的样本，选择的样本并没有标签，无法断定估计结果是否准确。
相比之下，自步学习则是选择预测值与真实值接近的、即分类器可以分辨的置信度真的高的样本。
> 此处有待继续仔细的品味

# 参考引用链接
[markdown latex语法](https://www.cnblogs.com/peaceWang/p/Markdown-tian-jia-Latex-shu-xue-gong-shi.html)
[SPL入门介绍](http://www.hanlongfei.com/机器学习/2017/07/24/selfpaced/)